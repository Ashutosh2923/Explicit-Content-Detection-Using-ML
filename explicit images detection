# -*- coding: utf-8 -*-
"""Explicit_images_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fcC6Zs3v42_bxBVZgeGjZeP2bh672oGA
"""

import pandas as pd
import numpy as np
import os
import keras
import matplotlib.pyplot as plt
from keras.layers import Dense,GlobalAveragePooling2D
from keras.applications import MobileNet
from keras.preprocessing import image
from keras.applications.mobilenet import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model
from keras.optimizers import Adam

base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.

x=base_model.output
x=GlobalAveragePooling2D()(x)
x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.
x=Dense(1024,activation='relu')(x) #dense layer 2
x=Dense(512,activation='relu')(x) #dense layer 3
preds=Dense(4,activation='softmax')(x) #final layer with softmax activation

model=Model(inputs=base_model.input,outputs=preds)
#specify the inputs
#specify the outputs
#now a model has been created based on our architecture

for layer in model.layers[:20]:
    layer.trainable=False
for layer in model.layers[20:]:
    layer.trainable=True

from google.colab import drive
drive.mount('/content/drive')

train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies

train_generator=train_datagen.flow_from_directory('/content/drive/MyDrive/Dataset', # this is where you specify the path to the main data folder
                                                 target_size=(224,224),
                                                 color_mode='rgb',
                                                 batch_size=32,
                                                 class_mode='categorical',
                                                 shuffle=True)

import PIL.Image
import glob
import os
from google.colab import files
import keras
from keras.preprocessing.image import ImageDataGenerator

# Create a function to identify and remove corrupted image files from the training dataset.
def remove_corrupted_images(image_files):
    for image_file in image_files:
        try:
            image = PIL.Image.open(image_file)
        except PIL.UnidentifiedImageError:
            print(f"Corrupted image file: {image_file}")
            if os.path.exists(image_file):
                os.remove(image_file)
            elif files.exists(image_file):
                files.delete(image_file)

# Get a list of all the image files in the training dataset directory.
image_files = glob.glob("/content/drive/MyDrive/Dataset/*.jpg")

# Identify and remove corrupted image files from the training dataset.
#remove_corrupted_images(image_files)

# Create a training data generator.
train_generator = ImageDataGenerator(rescale=1./255)
train_generator = train_generator.flow_from_directory(
    directory="/content/drive/MyDrive/Dataset",
    target_size=(224, 224),
    batch_size=32,
    class_mode="categorical"
)

# Calculate the number of steps per epoch.
step_size_train = train_generator.n // train_generator.batch_size

# Compile the model.
model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model for 5 epochs.
history=model.fit(train_generator, steps_per_epoch=step_size_train, epochs=5)

"""# Accuracy Of this Model is : **97.84%**"""

#---model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])
# Adam optimizer
# loss function will be categorical cross entropy
# evaluation metric will be accuracy

#---step_size_train=train_generator.n//train_generator.batch_size
#---model.fit_generator(generator=train_generator,
#                   steps_per_epoch=step_size_train,
 #                  epochs=5)

# Save the trained model.
model.save('model.h5')

import os

# Print the path to the model file.
print(os.path.abspath('model.h5'))

!pip install streamlit

import cv2
import numpy as np
from PIL import Image

from google.colab import files
uploaded = files.upload()

def predict_class(image_path, model):
  """Predicts the class of an image using a trained model.

  Args:
    image_path: The path to the image file.
    model: The trained model.

  Returns:
    The predicted class of the image.
  """

  # Open the image file.
  image = Image.open(image_path)

  # Resize the image to the expected size.
  image = image.resize((224, 224))

  # Preprocess the image.
  image = preprocess_input(np.array(image))

  # Make a prediction.
  predictions = model.predict(np.array([image]))

  # Get the predicted class.
  predicted_class = np.argmax(predictions)

  # Convert the predicted class to a string.
  class_name = {
    0: "Non-Explicit",
    1: "Explicit",
    2: "Explicit(guns)",
    3: "Explicit(pom)"

  }[predicted_class]

  # Return the predicted class name.
  return class_name


# Get the path to the uploaded image.
uploaded_image_path = list(uploaded.keys())[0]

# Predict the class of the uploaded image and display the output.
predicted_class_name = predict_class(uploaded_image_path, model)
print('The predicted class is:', predicted_class_name)

import pickle

with open('classifier.pkl','wb') as file:
  pickle.dump(model,file)

#from tensorflow import keras
#model = keras.models.load_model('model.h5')
#model.save('model_saved')

#model.save('/content/drive/MyDrive/model.h5')

import matplotlib.pyplot as plt

# Assuming you have the history object from your previous training
# This code will plot the training and validation accuracy
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()
